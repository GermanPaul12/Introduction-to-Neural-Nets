{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Neuronale Netze\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3d/Neural_network.svg\" alt=\"Ein Neuronales Netz (vereinfacht)\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EINLEITUNGSTEXT\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das einzelne Neuron\n",
    "<img src=\"https://miro.medium.com/max/856/1*O7YSSqlOdQuNgMrH7J_3dg.png\" alt=\"Das einzelne Neuron\" />\n",
    "\n",
    "Ein Neuronales Netz besteht aus vielen Neuronen, die miteinander verbunden sind. Dabei sind die Neuronen denen im Gehirn nachempfunden, welche untereinander verbunden sind und die aufeinander reagieren. Bei künstlichen Neuronalen Netzen hat jedes Neuron eine oder mehrere Eingaben $x_1...x_m$ und eine Ausgabe $o_j$. Die Eingaben werden mit Gewichten $w_1...w_m$ multipliziert und anschließend addiert ($ net_j = \\displaystyle\\sum_{i=1}^{m} (x_i \\cdot w_i) $). Manchmal gibt es noch einen Bias $b_j$ ($j$ steht für ein bestimmtes Neuron. Jedes Neuron hat einen eigenen Bias). Dieser wird auf diese Summe drauf addiert ($ net_j = \\displaystyle\\sum_{i=1}^{m} (x_i \\cdot w_i) + b_j $).Das Ergebnis wird dann in eine Aktivierungsfunktion $\\varphi_j(x)$  gegeben, welche das Ergebnis in die Ausgabe des Neurons umwandelt $o_j = \\varphi_j(net_j)$ und die dann ein Eingabewert (ein anderes $x$) für die nächsten Neuronen ist (oder die Ausgabe des Neuronalen Netzes darstellt, wenn das Neuron an der hintersten Ebene ist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein einfaches Beispiel \n",
    " \n",
    "Wir nehmen an wir haben ein Neuron mit zwei Inputs, welches die Sigmoid Funktion als Aktivierungsfunktion verwendet. Die folgenden Parameter sind gegeben:\n",
    "\n",
    "$ w = [0, 1] == w1 = 0\\ und\\ w2 = 1 \\\\\n",
    "$ b = 4\n",
    "\n",
    "Nun geben wir dem Neuron ein input x mit x = [2, 3]\n",
    "\n",
    "$$\n",
    "(w*x)+b = ((w1 * x1) + (w2 * x2)) + b \n",
    "        = 0 * 2 + 1 * 3 + 4\n",
    "        = 7\n",
    "y = f(w * x + b) = f(7) = 0.999       \n",
    "$$  \n",
    "\n",
    "Für den Input x = [2, 3] bekommen wir einen Output von 0,999. Dieser Prozess in welchem man inputs eingibt, um ein Output zu bekommen ist bekannt als **feedforward** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein Neuron programmieren\n",
    "\n",
    "Um das Neuron zu programmieren importieren wir Numpy. Numpy ist eine beliebte Python Library um mit Zahlen zu rechnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "#Wir importieren numpy als np um numpy nicht immer ausschreiben zu müssen\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Unsere Aktivierungsfunktion: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Gewichtsinputs, Addiere Gewichte und dann nutze die Aktivierungsfunktion\n",
    "    # np.dot ist eine Funktion um zwei Arrays zu multiplizieren \n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man sieht, kommen wir mit unserem selbstgecodeten Neuron auf dasselbe Ergebnis, wie zuvor nur mit Mathe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Überblick über die Aktivierungsfunktionen\n",
    "\n",
    "### Warum gibt es Aktivierungsfunktionen?\n",
    " Aktivierungsfunktionen werden verwendet, um dem neuronalen Netzwerk eine nicht lineare Eigenschaft zu verleihen. Auf diese Weise kann das Netz komplexere Beziehungen und Muster in den Daten modellieren. \n",
    "### Beispiele für Aktivierungsfunktionen\n",
    "<img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d244bb0e12c94fb442c01e_pasted%20image%200%20(4).jpg\" alt=\"Sigmoid-Aktiverungsfunktion\" height=\"300px\" />\n",
    "<img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24547f85f71e3bd2339f8_pasted%20image%200%20(5).jpg\" alt=\"Sigmoid-Aktiverungsfunktion\" height=\"300px\" />\n",
    "<img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24d1ac2cc1ded69730feb_relu.jpg\" alt=\"ReLu-Aktiverungsfunktion\" height=\"300px\" />\n",
    "\n",
    "noch beschreibenden Text einfügen..."
   ]
  },
  {
   "attachments": {
    "network.svg": {
     "image/svg+xml": [
      "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"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein einfaches Neuronales Netz \n",
    "\n",
    "Ein neuronales Netz ist im Endeffekt nur eine Verbidung vieler einzelner Neuronen. So kann ein einfaches Neuronales Netz aussehen:\n",
    "\n",
    "![https://victorzhou.com/77ed172fdef54ca1ffcfb0bba27ba334/network.svg](attachment:network.svg)\n",
    "\n",
    "Dieses Neuronale Netz hat zwei Inputs (x1 und x2), ein Hiddenlayer mit zwei Neuronen (h1 und h2) und ein Outputlayer mit einem Neuron (o1). \n",
    "\n",
    "Der Input für o1, sind die Outputs von h1 und h2.\n",
    "\n",
    "Ein Hiddenlayer ist jedes Layer zwischen dem ersten Layer dem Inputlayer und dem letzten Layer dem Outputlayer. Es kann mehrere Hiddenlayer geben.\n",
    "\n",
    "## Ein Beispiel für Feedforward\n",
    "\n",
    "Wir benutzen das Netzwerk von oben und nehmen an, dass alle Neuronen die selben Gewichte haben w = [0, 1], dasselbe gilt für den Bias b=0 und die selbe Aktivierungsfunktion Sigmoid. Nun schauen wir uns die Outputs für den Input x = [2, 3] an.\n",
    "\n",
    "$$\n",
    "h1 = h2 = f(w*x+b) \\\\ \n",
    "        = f((0 * 2) + (1 * 3) + 0) \\\\ \n",
    "        = f(3) \\\\ \n",
    "        = 0.9526\n",
    "$$\n",
    "\n",
    "$$\n",
    "o1 = f(w * [h1, h2] + b) \\\\\n",
    "= f((0 * h1) + (1 * h2) + 0) \\\\\n",
    "= f(0.9526) \\\\\n",
    "= 0.7216\n",
    "$$\n",
    "\n",
    "Der Output für den Input x = [2,3] ist 0.7216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein künstliches Neuronales Netz\n",
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Ein neuronales Netz\" width=\"400px\" />\n",
    "\n",
    "### Aufbau eines neuronalen Netzes\n",
    "\n",
    "Ein neuronales Netz besteht immer aus mindestens einer Schicht mit jeweils mindestens einem Neuron. Die zu verarbeitenden Daten werden in die erste Schicht neuronen gegeben (Input-Layer), wird dann durch das Netz verarbeitet und die letzte Schicht bildet ein oder mehr Neuronen die Ausgabe (die Anzahl von Neuronen pro Schicht - vor allem bei der Ein- und Ausgabeschicht - wird meistens vom Anwendungsfall bestimmt). Meistens sind alle Neuronen einer Schicht mit den Neuronen der nächsten Schicht verknüpft, d.h. die Ausgaben aller Neuronen einer Schicht sind Eingaben für die Neuronen der nächsten Schicht (das ist aber nicht zwingend so!). Bevor die Eingabewerte jedoch addiert werden und ihren Weg durch die Aktivierungsfunktion gehen, wird jede Ausgabe eines Neurons zu einem anderen mit einem eigenen Gewicht multipliziert. Dabei hat jede Verbindung ein eigenes Gewicht. Die Gewichte werden beim Training des neuronalen Netzes angepasst, sodass das Netz besser lernt, und eben diese Gewichte gilt es so zu wählen, dass das neuronale Netz die besten Ausgaben gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quellen\n",
    "\n",
    "\n",
    "1. <a href=\"https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\">Machine Learning for Beginners an Introduction to Neural Networks</a>\n",
    "2. <a href=\"https://learn.microsoft.com/de-de/training/modules/intro-machine-learning-keras/\">Coding Beispiel eines Neuronalen Netzes von Microsoft</a>\n",
    "3. <a href=\"https://open.hpi.de/courses/neuralnets2020/overview\">Online Kurs von OpenHPI zu Neuronalen Netzen</a>\n",
    "4. <a href=\"https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd\">Learn to write LaTeX in Jupyter Notebook</a>\n",
    "5. <a href=\"\"></a>\n",
    "\n",
    "\n",
    "\n",
    "### Dokumentation benutzter Libraries\n",
    "\n",
    "1. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras\">Keras</a>\n",
    "2. <a href=\"https://numpy.org/doc/stable/user/index.html#user\">Numpy</a>\n",
    "3. <a href=\"https://matplotlib.org/stable/api/pyplot_summary.html\">Matplotlib</a>\n",
    "4. <a href=\"\"></a>\n",
    "5. <a href=\"\"></a>\n",
    "\n",
    "### Empfehlungen bei zusätzlichem Literatur-/Informationsbedarf\n",
    "\n",
    "1. <a href=\"https://towardsdatascience.com/\">Towards Data Science Blog</a>\n",
    "2. <a href=\"\"></a>\n",
    "3. <a href=\"\"></a>\n",
    "4. <a href=\"\"></a>\n",
    "5. <a href=\"\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
